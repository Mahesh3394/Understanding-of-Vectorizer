{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnV82tg1xLi0"
      },
      "source": [
        "### Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "bUsYm9wjxLi1"
      },
      "outputs": [],
      "source": [
        "## SkLearn# Collection of string documents\n",
        "\n",
        "corpus = [\n",
        "     'this is the first document',\n",
        "     'this document is the second document',\n",
        "     'and this is the third one',\n",
        "     'is this the first document',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLwmFZfKxLi4"
      },
      "source": [
        "### SkLearn Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Np4dfQOkxLi4"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(corpus)\n",
        "skl_output = vectorizer.transform(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7Om8YpYxLi6",
        "outputId": "770fc9ff-e899-4ec4-9ff2-cbcf807557bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# sklearn feature names, they are sorted in alphabetic order by default.\n",
        "\n",
        "print(vectorizer.get_feature_names())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTKplK96xLi-",
        "outputId": "8c2259a8-22a5-4615-f0d1-db2c6f5f9946"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
            " 1.         1.91629073 1.        ]\n"
          ]
        }
      ],
      "source": [
        "# Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n",
        "# After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value.\n",
        "\n",
        "print(vectorizer.idf_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CTiWHygxLjA",
        "outputId": "6bf3a588-6609-49c1-b00e-8a718f6fbe2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4, 9)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# shape of sklearn tfidf vectorizer output after applying transform method.\n",
        "\n",
        "skl_output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDKEpbA-xLjD",
        "outputId": "a90d68a7-b755-4898-f966-d7855fc32d80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 8)\t0.38408524091481483\n",
            "  (0, 6)\t0.38408524091481483\n",
            "  (0, 3)\t0.38408524091481483\n",
            "  (0, 2)\t0.5802858236844359\n",
            "  (0, 1)\t0.46979138557992045\n"
          ]
        }
      ],
      "source": [
        "# sklearn tfidf values for first line of the above corpus.\n",
        "# Here the output is a sparse matrix\n",
        "\n",
        "print(skl_output[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QWo34hexLjF",
        "outputId": "2219aaab-8230-4798-906b-814526d63e04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ]
        }
      ],
      "source": [
        "# sklearn tfidf values for first line of the above corpus.\n",
        "# To understand the output better, here we are converting the sparse output matrix to dense matrix and printing it.\n",
        "# Notice that this output is normalized using L2 normalization. sklearn does this by default.\n",
        "\n",
        "print(skl_output[0].toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfIwx5LzxLjI"
      },
      "source": [
        "### Your custom implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "HjuCcJwXxLjJ"
      },
      "outputs": [],
      "source": [
        "# Compare your results with the above sklearn tfidf vectorizer\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from scipy.sparse import csr_matrix\n",
        "import math\n",
        "import operator\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "V3uIJcA2b1Vv"
      },
      "outputs": [],
      "source": [
        "# create a function \n",
        "\n",
        "def fit(dataset):\n",
        "  unique_words= set()\n",
        "\n",
        "  if isinstance(dataset,(list,)): #isinstance is used to check first argument is of class of secon arg-it is true or false\n",
        "    for row in dataset: #reviews\n",
        "      for word in row.split():\n",
        "        if len(word)<2: # to check any punctuations\n",
        "          continue\n",
        "        unique_words.add(word)\n",
        "    unique_words = sorted(list(unique_words))\n",
        "    vocab = {j:i for i,j in enumerate(unique_words)}\n",
        "\n",
        "    return vocab\n",
        "  else:\n",
        "    print(\"you need to pass list of sentance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUYyTyZHdy5T",
        "outputId": "7574e1ab-f6a0-4bae-eee2-161463755a30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'and': 0, 'document': 1, 'first': 2, 'is': 3, 'one': 4, 'second': 5, 'the': 6, 'third': 7, 'this': 8}\n"
          ]
        }
      ],
      "source": [
        "corpus = [\n",
        "     'this is the first document',\n",
        "     'this document is the second document',\n",
        "     'and this is the third one',\n",
        "     'is this the first document',\n",
        "]\n",
        "\n",
        "vocab=fit(corpus)\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ8HPeeDe92t",
        "outputId": "0bd08666-3999-478b-9188-e009976d352a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 8359.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 1)\t0.4697913855799205\n",
            "  (0, 2)\t0.580285823684436\n",
            "  (0, 3)\t0.3840852409148149\n",
            "  (0, 6)\t0.3840852409148149\n",
            "  (0, 8)\t0.3840852409148149\n",
            "  (1, 1)\t0.6876235979836937\n",
            "  (1, 3)\t0.2810886740337529\n",
            "  (1, 5)\t0.5386476208856762\n",
            "  (1, 6)\t0.2810886740337529\n",
            "  (1, 8)\t0.2810886740337529\n",
            "  (2, 0)\t0.511848512707169\n",
            "  (2, 3)\t0.267103787642168\n",
            "  (2, 4)\t0.511848512707169\n",
            "  (2, 6)\t0.267103787642168\n",
            "  (2, 7)\t0.511848512707169\n",
            "  (2, 8)\t0.267103787642168\n",
            "  (3, 1)\t0.4697913855799205\n",
            "  (3, 2)\t0.580285823684436\n",
            "  (3, 3)\t0.3840852409148149\n",
            "  (3, 6)\t0.3840852409148149\n",
            "  (3, 8)\t0.3840852409148149\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# create a transform function for it\n",
        "\n",
        "def transform(dataset,vocab):\n",
        "  rows = []\n",
        "  columns = []\n",
        "  values = []\n",
        "  idf= 0\n",
        "  final_tf = []\n",
        "  if isinstance(dataset, (list,)):\n",
        " \n",
        "    for idx, row in enumerate(tqdm(dataset)):\n",
        "      b=len(row.split())\n",
        "      c=len(dataset)\n",
        "      word_freq = dict((Counter(row.split())))\n",
        "      for word, freq in word_freq.items():\n",
        "        tf=(freq/b)\n",
        "        for i in dataset:\n",
        "          if word in i:\n",
        "            idf =idf+ 1\n",
        "        Num = 1 + c\n",
        "        Den = 1 + idf\n",
        "        idf_1 = (1 +(np.log(Num/Den)))\n",
        "        tf_idf = ((tf)*(idf_1))\n",
        "        idf=0 \n",
        "        if len(word) < 2:\n",
        "          continue\n",
        "        col_index = vocab.get(word, -1)\n",
        "\n",
        "        if col_index !=-1:\n",
        "          rows.append(idx)\n",
        "          columns.append(col_index)\n",
        "          values.append(tf_idf)\n",
        "    spa_mat= csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab)))\n",
        "    normalised_sparse= normalize(spa_mat, norm='l2', axis=1, copy=True, return_norm=False)\n",
        "    return normalised_sparse\n",
        "  else:\n",
        "    print(\"you need to pass list of strings\")       \n",
        "print(transform(corpus, vocab))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUBOBUOgVRLu",
        "outputId": "be963265-f8ad-415b-bec1-7087f3066eda"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 12255.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(transform(corpus, vocab).toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHxPLlwNxLjL",
        "outputId": "9a4f8576-a373-407e-f70f-9a32e0afa4f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Below is the code to load the cleaned_strings pickle file provided\n",
        "# Here corpus is of list type\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1BSRxOGWXeR",
        "outputId": "87aa40d4-3bba-4268-c8bc-e0ca1d61bbc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of documents in corpus =  746\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/cleaned_strings', 'rb') as f:\n",
        "    corpus = pickle.load(f)\n",
        "    \n",
        "# printing the length of the corpus loaded\n",
        "print(\"Number of documents in corpus = \",len(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZULfoOIdxLjQ",
        "outputId": "1a04fe39-bae6-4e3d-ead3-23c65cf7e01c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 746/746 [00:00<00:00, 967.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'aimless': 6.922918004572872, 'distressed': 6.922918004572872, 'drifting': 6.922918004572872, 'nearly': 6.922918004572872, 'attempting': 6.922918004572872, 'artiness': 6.922918004572872, 'gerardo': 6.922918004572872, 'emptiness': 6.922918004572872, 'messages': 6.922918004572872, 'buffet': 6.922918004572872, 'science': 6.922918004572872, 'teacher': 6.922918004572872, 'owls': 6.922918004572872, 'florida': 6.922918004572872, 'muppets': 6.922918004572872, 'overdue': 6.922918004572872, 'screenplay': 6.922918004572872, 'post': 6.922918004572872, 'practically': 6.922918004572872, 'structure': 6.922918004572872, 'tightly': 6.922918004572872, 'constructed': 6.922918004572872, 'vitally': 6.922918004572872, 'occurs': 6.922918004572872, 'content': 6.922918004572872, 'dozen': 6.922918004572872, 'highest': 6.922918004572872, 'superlative': 6.922918004572872, 'require': 6.922918004572872, 'puzzle': 6.922918004572872, 'solving': 6.922918004572872, 'fit': 6.922918004572872, 'pulls': 6.922918004572872, 'punches': 6.922918004572872, 'graphics': 6.922918004572872, 'insane': 6.922918004572872, 'massive': 6.922918004572872, 'unlockable': 6.922918004572872, 'properly': 6.922918004572872, 'rocks': 6.922918004572872, 'doomed': 6.922918004572872, 'conception': 6.922918004572872, 'minor': 6.922918004572872, 'changing': 6.922918004572872, 'confirm': 6.922918004572872, 'generic': 6.922918004572872, 'managed': 6.922918004572872, 'exaggerating': 6.922918004572872, 'trailer': 6.922918004572872, 'carrell': 6.922918004572872}\n",
            "{'aimless': 0, 'distressed': 1, 'drifting': 2, 'nearly': 3, 'attempting': 4, 'artiness': 5, 'gerardo': 6, 'emptiness': 7, 'messages': 8, 'buffet': 9, 'science': 10, 'teacher': 11, 'owls': 12, 'florida': 13, 'muppets': 14, 'overdue': 15, 'screenplay': 16, 'post': 17, 'practically': 18, 'structure': 19, 'tightly': 20, 'constructed': 21, 'vitally': 22, 'occurs': 23, 'content': 24, 'dozen': 25, 'highest': 26, 'superlative': 27, 'require': 28, 'puzzle': 29, 'solving': 30, 'fit': 31, 'pulls': 32, 'punches': 33, 'graphics': 34, 'insane': 35, 'massive': 36, 'unlockable': 37, 'properly': 38, 'rocks': 39, 'doomed': 40, 'conception': 41, 'minor': 42, 'changing': 43, 'confirm': 44, 'generic': 45, 'managed': 46, 'exaggerating': 47, 'trailer': 48, 'carrell': 49}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "def fit_50(dataset):\n",
        "  idf= 0\n",
        "  final_idf = {}\n",
        "  if isinstance(dataset, (list,)):\n",
        "    c=len(dataset)\n",
        "    for idx, row in enumerate(tqdm(dataset)): \n",
        "      word_freq = dict(Counter(row.split()))\n",
        "      for word, freq in word_freq.items():\n",
        "        for i in dataset:\n",
        "          if word in i:\n",
        "            idf =idf+ 1\n",
        "        Num = 1 + c\n",
        "        Den = 1 + idf\n",
        "        idf_1 = (1 +(np.log(Num/Den)))\n",
        "        final_idf.update({word:idf_1})\n",
        "        idf=0 \n",
        "    res = dict(sorted(final_idf.items(), key = itemgetter(1), reverse = True)[:50])\n",
        "    print(res)\n",
        "    vocab = {j:i for i,j in enumerate(res)}\n",
        "    return vocab\n",
        "  else:\n",
        "    print(\"you need to pass list of strings\") \n",
        "\n",
        "print(fit_50(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD0VZvwRx65N",
        "outputId": "90c85e96-1ef4-43e8-9768-95f623f8264d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 746/746 [00:00<00:00, 938.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'aimless': 6.922918004572872, 'distressed': 6.922918004572872, 'drifting': 6.922918004572872, 'nearly': 6.922918004572872, 'attempting': 6.922918004572872, 'artiness': 6.922918004572872, 'gerardo': 6.922918004572872, 'emptiness': 6.922918004572872, 'messages': 6.922918004572872, 'buffet': 6.922918004572872, 'science': 6.922918004572872, 'teacher': 6.922918004572872, 'owls': 6.922918004572872, 'florida': 6.922918004572872, 'muppets': 6.922918004572872, 'overdue': 6.922918004572872, 'screenplay': 6.922918004572872, 'post': 6.922918004572872, 'practically': 6.922918004572872, 'structure': 6.922918004572872, 'tightly': 6.922918004572872, 'constructed': 6.922918004572872, 'vitally': 6.922918004572872, 'occurs': 6.922918004572872, 'content': 6.922918004572872, 'dozen': 6.922918004572872, 'highest': 6.922918004572872, 'superlative': 6.922918004572872, 'require': 6.922918004572872, 'puzzle': 6.922918004572872, 'solving': 6.922918004572872, 'fit': 6.922918004572872, 'pulls': 6.922918004572872, 'punches': 6.922918004572872, 'graphics': 6.922918004572872, 'insane': 6.922918004572872, 'massive': 6.922918004572872, 'unlockable': 6.922918004572872, 'properly': 6.922918004572872, 'rocks': 6.922918004572872, 'doomed': 6.922918004572872, 'conception': 6.922918004572872, 'minor': 6.922918004572872, 'changing': 6.922918004572872, 'confirm': 6.922918004572872, 'generic': 6.922918004572872, 'managed': 6.922918004572872, 'exaggerating': 6.922918004572872, 'trailer': 6.922918004572872, 'carrell': 6.922918004572872}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 746/746 [00:00<00:00, 945.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 0)\t0.5773502691896257\n",
            "  (0, 1)\t0.5773502691896257\n",
            "  (0, 2)\t0.5773502691896257\n",
            "  (1, 3)\t1.0\n",
            "  (2, 4)\t0.7071067811865476\n",
            "  (2, 5)\t0.7071067811865476\n",
            "  (4, 6)\t1.0\n",
            "  (5, 7)\t1.0\n",
            "  (7, 8)\t1.0\n",
            "  (9, 9)\t0.5773502691896257\n",
            "  (9, 10)\t0.5773502691896257\n",
            "  (9, 11)\t0.5773502691896257\n",
            "  (10, 12)\t1.0\n",
            "  (11, 13)\t1.0\n",
            "  (12, 14)\t1.0\n",
            "  (16, 15)\t1.0\n",
            "  (17, 16)\t0.7071067811865475\n",
            "  (17, 17)\t0.7071067811865475\n",
            "  (18, 18)\t1.0\n",
            "  (19, 19)\t0.14142135623730948\n",
            "  (19, 20)\t0.14142135623730948\n",
            "  (19, 21)\t0.14142135623730948\n",
            "  (19, 22)\t0.14142135623730948\n",
            "  (19, 23)\t0.14142135623730948\n",
            "  (19, 24)\t0.14142135623730948\n",
            "  (19, 25)\t0.14142135623730948\n",
            "  (19, 26)\t0.14142135623730948\n",
            "  (19, 27)\t0.14142135623730948\n",
            "  (19, 28)\t0.14142135623730948\n",
            "  (19, 29)\t0.14142135623730948\n",
            "  (19, 30)\t0.14142135623730948\n",
            "  (19, 31)\t0.28284271247461895\n",
            "  (19, 32)\t0.14142135623730948\n",
            "  (19, 33)\t0.14142135623730948\n",
            "  (19, 34)\t0.4242640687119284\n",
            "  (19, 35)\t0.14142135623730948\n",
            "  (19, 36)\t0.4242640687119284\n",
            "  (19, 37)\t0.14142135623730948\n",
            "  (19, 38)\t0.14142135623730948\n",
            "  (19, 39)\t0.14142135623730948\n",
            "  (19, 40)\t0.14142135623730948\n",
            "  (19, 41)\t0.14142135623730948\n",
            "  (19, 42)\t0.14142135623730948\n",
            "  (19, 43)\t0.14142135623730948\n",
            "  (19, 44)\t0.14142135623730948\n",
            "  (19, 45)\t0.14142135623730948\n",
            "  (19, 46)\t0.14142135623730948\n",
            "  (19, 47)\t0.14142135623730948\n",
            "  (19, 48)\t0.14142135623730948\n",
            "  (19, 49)\t0.14142135623730948\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "vocab = fit_50(corpus)\n",
        "print(transform(corpus, vocab))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment_3_Instructions.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
